import streamlit as st
from openai import OpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAI
from langchain_core.output_parsers import StrOutputParser

from langchain_community.callbacks import get_openai_callback
from menu_streamlit import menu_with_redirect



if "messages" not in st.session_state:
    st.session_state.messages = []
    
if st.session_state.add_question == []  :
    st.session_state.add_question = ['']
    
def LLM_respond_Q(msg_log):
    """ì±„íŒ… ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ í•¨"""

    system_prompt = [("system", """You are a Korean doctor. you are going to ask the patient questions about his/her symptoms, ONE AT A TIME. 
ALL the questions in the list MUST be asked. HOWEVER, when the patient information doesn't match the content of the question, skip the question. For example, don't ask appenditis related question for patient with appendectomy history.
If all necessary questions in the question list have been asked, please output <|endofQuestion|>.:
    
    [question list]
    {question_list}                   
    """)]
    
    prompt_temp = system_prompt + msg_log
    
    prompt = ChatPromptTemplate.from_messages(prompt_temp)
    
    llm = ChatOpenAI(model_name="gpt-4o", temperature = 0) 
    output_parser = StrOutputParser()

    chain = prompt | llm | output_parser    
    
    output = chain.stream({"question_list": "\n ".join(st.session_state.add_question)})
    return output

def LLM_Summary(msg_log):
    """ì±„íŒ… ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ í•¨"""

    prompt = [("system", """Given a question and a chat log. Please summarize the what the patient said in the chat in order to recieve confirmation. Give the output in Korean and a list, only output the list.
                      
[Question]
{question}"""),
                    ("user", "[Chat log]\n{chat_log} ")]  


               
    
    user_type_mapping = {'human': '[patient]', 'ai': '[doctor]'}
    msg_log_text = "\n".join(f"{user_type_mapping[sender]} : {message}" for sender, message in msg_log)
    
    
    
    prompt = ChatPromptTemplate.from_messages(prompt)
    
    llm = ChatOpenAI(model_name="gpt-4o", temperature = 0) 
    output_parser = StrOutputParser()

    chain = prompt | llm | output_parser    
    
    output = chain.stream({"question": st.session_state.question, "chat_log": msg_log_text})
    return output
    
    
    
    
def demo():
    st.session_state.status = "chat"
    st.session_state.messages = [('ai', 'ë‡Œí˜ˆê´€ì¡°ì˜ìˆ ì„ ë°›ì€ ë³‘ì›ì—ì„œ ì‹œìˆ ì˜ ê¸´ê¸‰ì„±ì„ ì–´ë–»ê²Œ ì„¤ëª…í–ˆë‚˜ìš”?'), ('human', 'ì¦ìƒ ì—†ê³  ëª¨ì–‘ì´ ê´œì°®ì•„ì„œ 1ë…„ ë’¤ì— ë°›ì•„ë„ ê´œì°®ë‹¤ê³  í•˜ë”ë¼ê³ ìš”.'), ('ai', 'ë‹¤ë¥¸ ë³‘ì›ì—ì„œ ì‹œìˆ ì„ ë¹¨ë¦¬ ë°›ì„ ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ì…¨ë‚˜ìš”?'), ('human', 'ì•„ë‹ˆìš” ì•„ì§ì´ìš”'), ('ai', 'ë‹µë³€í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. í™˜ìë¶„ê»˜ì„œ ë§ì”€í•´ì£¼ì‹  ê²ƒì„ ìš”ì•½í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n1. 60ì„¸ ë‚¨ì\n2. ë‹¤ë¥¸ ì¦ìƒ ì—†ìŒ\n3. ê³ í˜ˆì•• ì•½, ê³ ì§€í˜ˆì¦ ì•½ ë³µìš© ì¤‘\n4. ë³‘ì›ì—ì„œ 1ë…„ ë’¤ ì‹œìˆ  ê°€ëŠ¥í•˜ë‹¤ê³  ì„¤ëª…\n5. ë‹¤ë¥¸ ë³‘ì›ì—ì„œ ì‹œìˆ  ì—¬ë¶€ëŠ” ì•„ì§ ì•Œì•„ë³´ì§€ ì•ŠìŒ\n\ní•´ë‹¹ ë‚´ìš©ì´ ë§ë‹¤ë©´ ë“±ë¡í•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.')]
    st.session_state.add_info = """1. 60ì„¸ ë‚¨ì
2. ë‹¤ë¥¸ ì¦ìƒ ì—†ìŒ
3. ê³ í˜ˆì•• ì•½, ê³ ì§€í˜ˆì¦ ì•½ ë³µìš© ì¤‘
4. ë³‘ì›ì—ì„œ 1ë…„ ë’¤ ì‹œìˆ  ê°€ëŠ¥í•˜ë‹¤ê³  ì„¤ëª…
5. ë‹¤ë¥¸ ë³‘ì›ì—ì„œ ì‹œìˆ  ì—¬ë¶€ëŠ” ì•„ì§ ì•Œì•„ë³´ì§€ ì•ŠìŒ"""
    st.switch_page('A_helper.py')

for message in st.session_state.messages:
    role = 'ğŸ©º' if message[0] == 'ai' else message[0]
    with st.chat_message(role):
        st.markdown(message[1])
        
        
if len(st.session_state.messages) ==0 : 
    st.session_state.messages.append(('ai','ì˜ì‚¬ ì„ ìƒë‹˜ê»˜ì„œ ë” ìì„¸í•˜ê³  ë‹µë³€ì„ í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ ëª‡ê°€ì§€ ì¶”ê°€ë¡œ ì—¬ì­™ê² ìŠµë‹ˆë‹¤.\n\n'+st.session_state.add_question[0]))
    
    with st.chat_message("ğŸ©º"):
        st.markdown('ì˜ì‚¬ ì„ ìƒë‹˜ê»˜ì„œ ë” ìì„¸í•˜ê³  ë‹µë³€ì„ í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ ëª‡ê°€ì§€ ì¶”ê°€ë¡œ ì—¬ì­™ê² ìŠµë‹ˆë‹¤.\n\n'+st.session_state.add_question[0])

        
if userinput := st.chat_input("ë‹µë³€ì„ ì ì–´ì£¼ì„¸ìš”"):
    # Add user message to chat history
    st.session_state.messages.append(("human", userinput))
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(userinput)
        
    if len(st.session_state.messages) < len(st.session_state.add_question)*2 :
    # Display assistant response in chat message container
        with st.chat_message("ğŸ©º"):
            stream = LLM_respond_Q(st.session_state.messages)
            response = st.write_stream(stream)
        st.session_state.messages.append(("ai", response))
    
    else :
        summary = LLM_Summary(st.session_state.messages)
        with st.chat_message("ğŸ©º"):
            st.write('ë‹µë³€í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. í™˜ìë¶„ê»˜ì„œ ë§ì”€í•´ì£¼ì‹  ê²ƒì„ ìš”ì•½í•´ë³´ê² ìŠµë‹ˆë‹¤.')
            st.session_state.additional_information = st.write_stream(summary)
            st.write('í•´ë‹¹ ë‚´ìš©ì´ ë§ë‹¤ë©´ ë“±ë¡í•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.')

        st.session_state.messages.append(('ai','ë‹µë³€í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. í™˜ìë¶„ê»˜ì„œ ë§ì”€í•´ì£¼ì‹  ê²ƒì„ ìš”ì•½í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n'+st.session_state.additional_information + '\n\ní•´ë‹¹ ë‚´ìš©ì´ ë§ë‹¤ë©´ ë“±ë¡í•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.'))
        st.page_link("pages/A_helper.py", label="ë“±ë¡í•˜ê¸°")

    
    


with st.sidebar:
    st.button("Demo",on_click=demo)
menu_with_redirect()
