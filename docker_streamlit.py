import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAI
from langchain_core.output_parsers import StrOutputParser, NumberedListOutputParser, CommaSeparatedListOutputParser
from langchain_community.callbacks import get_openai_callback
from menu_streamlit import menu_with_redirect
from langchain_core.runnables import (
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
)
import os
from langsmith import traceable



if "messages" not in st.session_state:
    st.session_state.messages = []
    
if 'add_question' not in st.session_state:
    st.session_state.add_question = ['']
    
if 'question' not in st.session_state:
    st.session_state.question = ''
    

def LLM_respond_Q(msg_log):
    """ì±„íŒ… ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ í•¨"""

    system_prompt = [("system", """You are a Korean doctor. you are going to ask the patient additional questions needed to retrieve information to answer his/her initial question. Ask one question at a time. 
ALL the questions in the list MUST be asked. 
HOWEVER, when the patient information doesn't match the content of the question, skip the question. For example, don't ask appenditis related question for patient with appendectomy history.
If there are no questions in the question list. Assume no additional questions are needed. 
If there are no more questions to be asked, please output <|endofQuestion|>. Do not answer the initial question.:
    
    [initial question]
    {question}
    
    [question list]
    {question_list}                   
    """)]
    
    prompt_temp = system_prompt + msg_log
    
    prompt = ChatPromptTemplate.from_messages(prompt_temp)
    
    llm = ChatOpenAI(model_name="gpt-4o", temperature = 0) 
    output_parser = StrOutputParser()

    chain = prompt | llm | output_parser    
    
    output = chain.stream({"question" : st.session_state.question, "question_list": "\n ".join(st.session_state.add_question)})
    return output

def LLM_Summary(msg_log):
    """ì±„íŒ… ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ í•¨"""

    system_prompt = [("system", """You are a Korean doctor. you are going to ask the patient additional questions needed to retrieve information to answer his/her initial question. Ask one question at a time. 
ALL the questions in the list MUST be asked. 
HOWEVER, when the patient information doesn't match the content of the question, skip the question. For example, don't ask appenditis related question for patient with appendectomy history.
If there are no questions in the question list. Assume no additional questions are needed. 
If there are no more questions to be asked, please output <|endofQuestion|>. Do not answer the initial question.
When asked to summarize, please summarize what the patient said in the chat. Give the output in Korean and a list, only output the list.:
    
    [initial question]
    {question}
    
    [question list]
    {question_list}                   
    """),
                    ]  

    prompt_temp = system_prompt + msg_log + [("user", "summarize")]
               
    
    
    
    prompt = ChatPromptTemplate.from_messages(prompt_temp)
    
    llm = ChatOpenAI(model_name="gpt-4o", temperature = 0) 
    output_parser = StrOutputParser()

    chain = prompt | llm | output_parser    
    
    output = chain.stream({"question": st.session_state.question, "question_list": "\n ".join(st.session_state.add_question)})
    return output
    
def question_generator(model):
    number_output_parser = NumberedListOutputParser()
    format_instructions=number_output_parser.get_format_instructions()
    prompt_addQ = ChatPromptTemplate.from_messages([
        ("system", "You are a Korean doctor consulting a patient. When given a medical complaint, ask questions that would help you answer the question. The maximum number of questions should be 5. If no question need to be asked, output empty list. \n{}".format(format_instructions)),
        ("user", "\n{question}")
    ])

    prompt_verify = ChatPromptTemplate.from_messages([
        ("system", "Verify whether a question is a medical related question or not. Simply, output T if it's medical related and F if it isn't. no need to explain"),
        ("user", "\n{question}")
    ])

    llm4o = ChatOpenAI(model_name=model, temperature = 0)





    chain_addQ = prompt_addQ | llm4o | number_output_parser
    chain_verify = prompt_verify | llm4o | StrOutputParser()

    chain = RunnableParallel(
        addQ = chain_addQ,
        verify = chain_verify
    ) 
    return chain
    
    
def demo():
    st.session_state.status = "chat"
    st.session_state.messages = []
    
def refresh():
    st.session_state.messages =[]
    st.rerun()
    
    
@st.experimental_dialog("ì£„ì†¡í•©ë‹ˆë‹¤.")
def throw_error(verify_result):
    if verify_result == "F":
        st.write("ë‹¥í„°í”Œë ‰ìŠ¤ëŠ” ê±´ê°• ë° ì˜í•™ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        if st.button("ë‹¤ì‹œ ì§ˆë¬¸í•˜ê¸°") :
            st.session_state.messages = []
            st.rerun()
    if verify_result == "T":
        st.rerun()

for message in st.session_state.messages:
    role = 'ğŸ©º' if message[0] == 'ai' else message[0]
    with st.chat_message(role):
        st.markdown(message[1])
        
        
        
if len(st.session_state.messages) ==0 : 
    #print(st.session_state.add_question)
    st.session_state.messages.append(('ai','ì•ˆë…•í•˜ì„¸ìš”? ë‹¥í„°í”Œë ‰ìŠ¤ì…ë‹ˆë‹¤. ê±´ê°•ìƒ ê°„ë‹¨í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì˜ì‚¬ì„ ìƒë‹˜ì´ ë‹µë³€ì„ ë“œë¦½ë‹ˆë‹¤.' ))
    st.session_state.messages.append(('ai','ì–´ë–¤ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? ìì„¸í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì˜ì‚¬ ì„ ìƒë‹˜ê»˜ì„œ ë” ì˜ ë‹µë³€ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' ))
    
    with st.chat_message("ğŸ©º"):
        st.markdown('ì•ˆë…•í•˜ì„¸ìš”?  ë‹¥í„°í”Œë ‰ìŠ¤ ì…ë‹ˆë‹¤. ê±´ê°•ìƒ ê°„ë‹¨í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì˜ì‚¬ì„ ìƒë‹˜ì´ ë‹µë³€ì„ ë“œë¦½ë‹ˆë‹¤.')
    with st.chat_message("ğŸ©º"):
        st.markdown('ì–´ë–¤ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? ìì„¸í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì˜ì‚¬ ì„ ìƒë‹˜ê»˜ì„œ ë” ì˜ ë‹µë³€ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' )

        
if userinput := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    # Add user message to chat history
    st.session_state.messages.append(("human", userinput))
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(userinput)
        
        
    if len(st.session_state.messages) == 3 :
        st.session_state.question  = userinput
        response = question_generator('gpt-4o').invoke(st.session_state.question)
        st.session_state.add_question = response['addQ']
        if response['verify'] == "F":
            throw_error(response['verify'])
            st.stop()
            
    
    # Display assistant response in chat message container
    with st.chat_message("ğŸ©º"):
        stream = LLM_respond_Q(st.session_state.messages)
        response = st.write_stream(stream)
        if response.strip() != '<|endofQuestion|>' :
            st.session_state.messages.append(("ai", response))
    
        else :
            summary = LLM_Summary(st.session_state.messages)
            st.write('ë‹µë³€í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. í™˜ìë¶„ê»˜ì„œ ë§ì”€í•´ì£¼ì‹  ê²ƒì„ ìš”ì•½í•´ë³´ê² ìŠµë‹ˆë‹¤.')
            st.session_state.additional_information = st.write_stream(summary)
            st.write('í•´ë‹¹ ë‚´ìš©ì´ ë§ë‹¤ë©´ ë“±ë¡í•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.')

            st.session_state.messages.append(('ai','ë‹µë³€í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. í™˜ìë¶„ê»˜ì„œ ë§ì”€í•´ì£¼ì‹  ê²ƒì„ ìš”ì•½í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n'+st.session_state.additional_information + '\n\ní•´ë‹¹ ë‚´ìš©ì´ ë§ë‹¤ë©´ ë“±ë¡í•˜ê¸° ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.'))
            st.page_link("pages/docker_A_helper.py",label="ë“±ë¡í•˜ê¸°")
            
            # if st.button("ë“±ë¡í•˜ê¸°") :
            #     st.info("ì§ˆë¬¸ì´ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âœ…")
            #     st.switch_page("docker_A_helper.py")
    
    
